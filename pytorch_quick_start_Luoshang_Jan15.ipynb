{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoshanPAN/colab_notebooks/blob/main/pytorch_quick_start_Luoshang_Jan15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XHd5ExbUIUg"
      },
      "source": [
        "# PyTorch 1.2 Quickstart with Google Colab\n",
        "In this code tutorial we will learn how to quickly train a model to understand some of PyTorch's basic building blocks to train a deep learning model. This notebook is inspired by the [\"Tensorflow 2.0 Quickstart for experts\"](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb#scrollTo=DUNzJc4jTj6G) notebook. \n",
        "\n",
        "After completion of this tutorial, you should be able to import data, transform it, and efficiently feed the data in batches to a convolution neural network (CNN) model for image classification.\n",
        "\n",
        "**Author:** [Elvis Saravia](https://twitter.com/omarsar0)\n",
        "\n",
        "**Complete Code Walkthrough:** [Blog post](https://medium.com/dair-ai/pytorch-1-2-quickstart-with-google-colab-6690a30c38d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzsiN3l_Vy1p",
        "outputId": "62cf518f-2c06-4c27-cc84-e4b16f067480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !pip3 uninstall torchvision\n",
        "!pip3 install torch==1.13.0+cu116  -f https://download.pytorch.org/whl/torch_stable.html # torchvision==0.9.1+cu111\n",
        "!pip3 install torchrec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.13.0+cu116 in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0+cu116) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchrec\n",
            "  Downloading torchrec-0.3.2-py38-none-any.whl (317 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.6/317.6 KB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython==0.29.32 in /usr/local/lib/python3.8/dist-packages (from torchrec) (0.29.32)\n",
            "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from torchrec) (2022.12.7)\n",
            "Collecting fbgemm-gpu==0.3.2\n",
            "  Downloading fbgemm_gpu-0.3.2-cp38-cp38-manylinux1_x86_64.whl (224.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyDeprecate==0.3.2\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Collecting websocket-client==1.4.2\n",
            "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics==0.11.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions==0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting MarkupSafe==2.1.1\n",
            "  Downloading MarkupSafe-2.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting cmake==3.25.0\n",
            "  Downloading cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec==2022.11.0 in /usr/local/lib/python3.8/dist-packages (from torchrec) (2022.11.0)\n",
            "Collecting pyre-extensions==0.0.27\n",
            "  Downloading pyre_extensions-0.0.27-py3-none-any.whl (12 kB)\n",
            "Collecting typing-inspect==0.8.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from torchrec) (4.64.1)\n",
            "Collecting ninja==1.11.1\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs==22.1.0\n",
            "  Downloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions==4.4.0 in /usr/local/lib/python3.8/dist-packages (from torchrec) (4.4.0)\n",
            "Collecting filelock==3.8.2\n",
            "  Downloading filelock-3.8.2-py3-none-any.whl (10 kB)\n",
            "Collecting tabulate==0.9.0\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting torchx==0.3.0\n",
            "  Downloading torchx-0.3.0-py3-none-any.whl (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docstring-parser==0.8.1\n",
            "  Downloading docstring_parser-0.8.1.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting idna==3.4\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-build==0.16.3\n",
            "  Downloading scikit_build-0.16.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow==10.0.1\n",
            "  Downloading pyarrow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.0/36.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting arrow==1.2.3\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker==2.6.0\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting urllib3==1.26.13\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer==2.1.1 in /usr/local/lib/python3.8/dist-packages (from torchrec) (2.1.1)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.8/dist-packages (from torchrec) (2.8.2)\n",
            "Collecting usort==1.0.5\n",
            "  Downloading usort-1.0.5-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pyparsing==3.0.9 in /usr/local/lib/python3.8/dist-packages (from torchrec) (3.0.9)\n",
            "Collecting docker==6.0.1\n",
            "  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.5/147.5 KB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six==1.16.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting distro==1.8.0\n",
            "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
            "Collecting Jinja2==3.1.2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 KB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging==22.0\n",
            "  Downloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.8/dist-packages (from torchrec) (6.0)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.8/dist-packages (from torchrec) (1.3.5)\n",
            "Collecting iopath==0.1.10\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytz==2022.6\n",
            "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 KB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.8/dist-packages (from torchrec) (1.21.6)\n",
            "Collecting hypothesis==6.61.0\n",
            "  Downloading hypothesis-6.61.0-py3-none-any.whl (398 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m398.6/398.6 KB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.8/dist-packages (from torchrec) (2.4.0)\n",
            "Collecting requests==2.28.1\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exceptiongroup>=1.0.0\n",
            "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: wheel>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from scikit-build==0.16.3->torchrec) (0.38.4)\n",
            "Requirement already satisfied: setuptools>=42.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-build==0.16.3->torchrec) (57.4.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics==0.11.0->torchrec) (1.13.0+cu116)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from torchx==0.3.0->torchrec) (6.0.0)\n",
            "Collecting moreorless>=0.3.0\n",
            "  Downloading moreorless-0.4.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: toml>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from usort==1.0.5->torchrec) (0.10.2)\n",
            "Collecting trailrunner>=1.0\n",
            "  Downloading trailrunner-1.2.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: click>=7.0.0 in /usr/local/lib/python3.8/dist-packages (from usort==1.0.5->torchrec) (7.1.2)\n",
            "Collecting LibCST>=0.3.7\n",
            "  Downloading libcst-0.4.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stdlibs>=2021.4.1\n",
            "  Downloading stdlibs-2022.10.9-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathspec>=0.8.1\n",
            "  Downloading pathspec-0.10.3-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->torchx==0.3.0->torchrec) (3.11.0)\n",
            "Building wheels for collected packages: docstring-parser, iopath\n",
            "  Building wheel for docstring-parser (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docstring-parser: filename=docstring_parser-0.8.1-py3-none-any.whl size=19678 sha256=a6457827cd1ccf98fdbd579416abe00e204f014b38199bd73b73d94fee9ad258\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/06/c4/9b2f0146899e8d1e7748897e42ad5412b6a025513f89cc4a0f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=fc4c46d27ddc2346fa0eb8b39c88e7fbbe108bc9b8126bf710fb42e19b626cf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/3e/24/0f349c0b2eeb6965903035f3b00dbb5c9bea437b4a2f18d82c\n",
            "Successfully built docstring-parser iopath\n",
            "Installing collected packages: pytz, ninja, mypy-extensions, fbgemm-gpu, cmake, websocket-client, urllib3, typing-inspect, tabulate, stdlibs, six, pyDeprecate, pyarrow, portalocker, pathspec, packaging, moreorless, MarkupSafe, idna, filelock, exceptiongroup, docstring-parser, distro, attrs, trailrunner, torchmetrics, scikit-build, requests, pyre-extensions, LibCST, Jinja2, iopath, hypothesis, usort, docker, arrow, torchx, torchrec\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2022.7\n",
            "    Uninstalling pytz-2022.7:\n",
            "      Successfully uninstalled pytz-2022.7\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.22.6\n",
            "    Uninstalling cmake-3.22.6:\n",
            "      Successfully uninstalled cmake-3.22.6\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.8.10\n",
            "    Uninstalling tabulate-0.8.10:\n",
            "      Successfully uninstalled tabulate-0.8.10\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 9.0.0\n",
            "    Uninstalling pyarrow-9.0.0:\n",
            "      Successfully uninstalled pyarrow-9.0.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.0.1\n",
            "    Uninstalling MarkupSafe-2.0.1:\n",
            "      Successfully uninstalled MarkupSafe-2.0.1\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.9.0\n",
            "    Uninstalling filelock-3.9.0:\n",
            "      Successfully uninstalled filelock-3.9.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 22.2.0\n",
            "    Uninstalling attrs-22.2.0:\n",
            "      Successfully uninstalled attrs-22.2.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 10.0.1 which is incompatible.\n",
            "notebook 5.7.16 requires jinja2<=3.0.0, but you have jinja2 3.1.2 which is incompatible.\n",
            "google-cloud-bigquery 3.4.1 requires packaging<22.0.0dev,>=14.3, but you have packaging 22.0 which is incompatible.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Jinja2-3.1.2 LibCST-0.4.9 MarkupSafe-2.1.1 arrow-1.2.3 attrs-22.1.0 cmake-3.25.0 distro-1.8.0 docker-6.0.1 docstring-parser-0.8.1 exceptiongroup-1.1.0 fbgemm-gpu-0.3.2 filelock-3.8.2 hypothesis-6.61.0 idna-3.4 iopath-0.1.10 moreorless-0.4.0 mypy-extensions-0.4.3 ninja-1.11.1 packaging-22.0 pathspec-0.10.3 portalocker-2.6.0 pyDeprecate-0.3.2 pyarrow-10.0.1 pyre-extensions-0.0.27 pytz-2022.6 requests-2.28.1 scikit-build-0.16.3 six-1.16.0 stdlibs-2022.10.9 tabulate-0.9.0 torchmetrics-0.11.0 torchrec-0.3.2 torchx-0.3.0 trailrunner-1.2.1 typing-inspect-0.8.0 urllib3-1.26.13 usort-1.0.5 websocket-client-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF1P_cRoWpvM"
      },
      "source": [
        "Note: We will be using the latest stable version of PyTorch so be sure to run the command above to install the latest version of PyTorch, which as the time of this tutorial was 1.2.0. We PyTorch belowing using the `torch` module. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su0COdCqT2Wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd45a71-3dd6-48a9-80bf-035a21801918"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchrec\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXCYmmjyVRq5"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchrec\n",
        "import torch.distributed as dist\n",
        "\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
        "\n",
        "# Note - you will need a V100 or A100 to run tutorial as as!\n",
        "# If using an older GPU (such as colab free K80), \n",
        "# you will need to compile fbgemm with the appripriate CUDA architecture\n",
        "# or run with \"gloo\" on CPUs \n",
        "dist.init_process_group(backend=\"nccl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ebc = torchrec.EmbeddingBagCollection(\n",
        "    device=\"meta\",\n",
        "    tables=[\n",
        "        torchrec.EmbeddingBagConfig(\n",
        "            name=\"product_table\",\n",
        "            embedding_dim=64,\n",
        "            num_embeddings=4096,\n",
        "            feature_names=[\"product\"],\n",
        "            pooling=torchrec.PoolingType.SUM,\n",
        "        ),\n",
        "        torchrec.EmbeddingBagConfig(\n",
        "            name=\"user_table\",\n",
        "            embedding_dim=64,\n",
        "            num_embeddings=4096,\n",
        "            feature_names=[\"user\"],\n",
        "            pooling=torchrec.PoolingType.SUM,\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "IUNypP6DUJ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchrec.optim.apply_overlapped_optimizer import apply_overlapped_optimizer\n",
        "\n",
        "apply_overlapped_optimizer(\n",
        "    ebc.parameters(),\n",
        "    optimizer_type=torch.optim.SGD,\n",
        "    optimizer_kwargs={\"lr\": 0.02},\n",
        ")"
      ],
      "metadata": {
        "id": "MN8XB-MRULPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchrec.distributed.DistributedModelParallel(ebc, device=torch.device(\"cuda\"))\n",
        "print(model)\n",
        "print(model.plan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bo431Jjbg2j",
        "outputId": "4410c979-9c5b-416a-d9b2-71ffa0cd5f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Could not determine LOCAL_WORLD_SIZE from environment, falling back to WORLD_SIZE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistributedModelParallel(\n",
            "  (_dmp_wrapped_module): ShardedEmbeddingBagCollection(\n",
            "    (lookups): \n",
            "     GroupedPooledEmbeddingsLookup(\n",
            "        (_emb_modules): ModuleList(\n",
            "          (0): BatchedFusedEmbeddingBag(\n",
            "            (_emb_module): SplitTableBatchedEmbeddingBagsCodegen()\n",
            "          )\n",
            "        )\n",
            "        (_score_emb_modules): ModuleList()\n",
            "      )\n",
            "     (_output_dists): \n",
            "     TwPooledEmbeddingDist(\n",
            "        (_dist): PooledEmbeddingsAllToAll()\n",
            "      )\n",
            "    (_input_dists): ModuleList()\n",
            "    (_lookups): ModuleList(\n",
            "      (0): GroupedPooledEmbeddingsLookup(\n",
            "        (_emb_modules): ModuleList(\n",
            "          (0): BatchedFusedEmbeddingBag(\n",
            "            (_emb_module): SplitTableBatchedEmbeddingBagsCodegen()\n",
            "          )\n",
            "        )\n",
            "        (_score_emb_modules): ModuleList()\n",
            "      )\n",
            "    )\n",
            "    (_output_dists): ModuleList(\n",
            "      (0): TwPooledEmbeddingDist(\n",
            "        (_dist): PooledEmbeddingsAllToAll()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "{'': {'product_table': ParameterSharding(sharding_type='table_wise', compute_kernel='fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:0/cuda:0)])), 'user_table': ParameterSharding(sharding_type='table_wise', compute_kernel='fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:0/cuda:0)]))}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchrec.distributed.fbgemm_qcomm_codec import get_qcomm_codecs_registry, QCommsConfig, CommType\n",
        "from torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder\n",
        "\n",
        "sharder = EmbeddingBagCollectionSharder(\n",
        "    qcomm_codecs_registry=get_qcomm_codecs_registry(\n",
        "            qcomms_config=QCommsConfig(\n",
        "                forward_precision=CommType.FP16,\n",
        "                backward_precision=CommType.BF16,\n",
        "            )\n",
        "        )\n",
        ")\n",
        "model = torchrec.distributed.DistributedModelParallel(ebc, sharders=[sharder], device=torch.device(\"cuda\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czJAz1l7bhA0",
        "outputId": "d5338228-8576-4e23-8d5a-acf4ba2b3004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Could not determine LOCAL_WORLD_SIZE from environment, falling back to WORLD_SIZE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product_eb = torch.nn.EmbeddingBag(4096, 64)\n",
        "product_eb(input=torch.tensor([101, 202, 303]), offsets=torch.tensor([0, 2, 2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81jQhXT_bmda",
        "outputId": "3346a540-3bbd-4eac-e561-24a0fc5df892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.6645, -1.4839, -0.0569,  0.8944,  2.4186,  0.0227,  1.0029, -0.6602,\n",
              "          1.0186, -0.7351, -1.1619, -1.4583, -0.6978,  1.3495,  0.7533, -0.2423,\n",
              "         -0.0375,  0.3004, -0.1910,  1.4390, -0.4510,  1.2134,  0.5641, -1.1812,\n",
              "         -0.4082, -0.1815, -0.4184,  0.1335, -0.1494, -1.1686,  0.0927, -0.5570,\n",
              "         -0.3412,  0.3418, -0.1518, -0.1652, -0.2961, -0.5735,  0.3383, -0.1959,\n",
              "          0.1078, -0.2904,  0.0960,  0.3101, -0.1779, -0.3172, -0.2015,  0.3363,\n",
              "         -0.7161, -0.9023, -0.5943, -0.0071, -0.1969,  1.7662, -0.4355,  0.6728,\n",
              "         -0.1358, -0.6523, -0.1524, -0.3256,  0.9686, -0.4752,  0.8857,  0.1342],\n",
              "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
              "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.6383, -0.1286,  0.1336, -0.1927,  1.3680, -1.5715,  1.5829,  1.1606,\n",
              "          0.7077, -0.0173, -0.8231, -1.1695,  0.5876,  0.4420, -0.8936, -0.3119,\n",
              "          1.1950,  0.6051, -0.4583, -0.2051,  1.4315,  0.8721,  0.4513,  0.4879,\n",
              "         -0.2750,  0.6569,  1.6486, -0.3943,  1.5019,  0.3851,  1.4890, -0.4364,\n",
              "         -0.2300,  0.6970, -0.4708,  0.4419, -0.7214, -1.2193, -1.0920,  0.4620,\n",
              "         -1.0282,  2.1411,  0.5028, -0.0455,  1.2495,  0.0823, -0.8456,  1.4668,\n",
              "         -0.2578, -0.0927, -0.4065,  1.1068, -0.1629, -0.7375, -1.8368,  0.1323,\n",
              "         -0.6216, -0.1490, -2.5027,  0.8838,  2.6100, -0.1575,  2.4943,  0.0963]],\n",
              "       grad_fn=<EmbeddingBagBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mb = torchrec.KeyedJaggedTensor(\n",
        "    keys = [\"product\", \"user\"],\n",
        "    values = torch.tensor([101, 202, 303, 404, 505, 606]).cuda(),\n",
        "    lengths = torch.tensor([2, 0, 1, 1, 1, 1], dtype=torch.int64).cuda(),\n",
        ")\n",
        "\n",
        "print(mb.to(torch.device(\"cpu\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi5f-ulGbrlp",
        "outputId": "5ed6cf73-d202-4a54-dcd0-b8f5fc0c3dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KeyedJaggedTensor({\n",
            "    \"product\": [[101, 202], [], [303]],\n",
            "    \"user\": [[404], [505], [606]]\n",
            "})\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_embeddings = model(mb).to_dict()\n",
        "print(\"product embeddings\", pooled_embeddings[\"product\"])\n",
        "print(\"user embeddings\", pooled_embeddings[\"user\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1Mr_s5obwpV",
        "outputId": "7a90f6a0-b34e-4c43-c92d-b3d700a3835f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "product embeddings tensor([[-0.0057, -0.0044,  0.0004,  0.0023,  0.0051, -0.0142,  0.0002, -0.0221,\n",
            "          0.0069,  0.0084,  0.0024,  0.0075,  0.0092,  0.0077,  0.0095, -0.0054,\n",
            "          0.0183, -0.0143,  0.0003, -0.0170, -0.0213, -0.0233,  0.0124, -0.0156,\n",
            "          0.0019, -0.0145, -0.0147, -0.0091, -0.0270, -0.0104,  0.0031,  0.0195,\n",
            "         -0.0001,  0.0179, -0.0005,  0.0278,  0.0020,  0.0243, -0.0187,  0.0098,\n",
            "          0.0077, -0.0079, -0.0133, -0.0047, -0.0031,  0.0186,  0.0211,  0.0101,\n",
            "          0.0037,  0.0052,  0.0087, -0.0227,  0.0216, -0.0035,  0.0076,  0.0174,\n",
            "         -0.0028,  0.0006,  0.0057, -0.0160,  0.0019,  0.0268, -0.0010,  0.0022],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0123,  0.0151, -0.0140, -0.0003, -0.0078,  0.0120, -0.0004, -0.0006,\n",
            "          0.0006, -0.0024,  0.0014, -0.0035,  0.0107, -0.0085, -0.0085, -0.0014,\n",
            "          0.0114,  0.0153, -0.0033,  0.0078,  0.0156,  0.0097, -0.0063, -0.0066,\n",
            "          0.0044, -0.0121,  0.0033, -0.0071,  0.0028,  0.0003,  0.0093, -0.0150,\n",
            "          0.0150,  0.0106,  0.0126,  0.0051,  0.0138,  0.0072,  0.0151,  0.0032,\n",
            "         -0.0017, -0.0142,  0.0146, -0.0001,  0.0061, -0.0133,  0.0085, -0.0014,\n",
            "          0.0029, -0.0072, -0.0067,  0.0123,  0.0111, -0.0043,  0.0052, -0.0145,\n",
            "         -0.0046, -0.0004,  0.0060, -0.0009, -0.0053,  0.0079, -0.0103, -0.0138]],\n",
            "       device='cuda:0', grad_fn=<SplitWithSizesBackward0>)\n",
            "user embeddings tensor([[ 0.0146,  0.0058,  0.0074,  0.0077,  0.0126, -0.0150,  0.0074,  0.0060,\n",
            "         -0.0048, -0.0112,  0.0014,  0.0091,  0.0119, -0.0126,  0.0141, -0.0137,\n",
            "          0.0068,  0.0013, -0.0078, -0.0002, -0.0094,  0.0013,  0.0009, -0.0038,\n",
            "         -0.0089, -0.0119,  0.0132,  0.0154,  0.0136, -0.0008, -0.0147,  0.0055,\n",
            "          0.0145, -0.0017,  0.0151,  0.0004, -0.0101, -0.0108, -0.0089, -0.0100,\n",
            "          0.0016,  0.0153,  0.0058,  0.0058,  0.0088,  0.0106, -0.0093, -0.0087,\n",
            "         -0.0053, -0.0129,  0.0047, -0.0045, -0.0063,  0.0076,  0.0098,  0.0113,\n",
            "         -0.0147,  0.0128,  0.0131, -0.0130, -0.0058,  0.0131, -0.0047, -0.0012],\n",
            "        [ 0.0097, -0.0032, -0.0113, -0.0088,  0.0020,  0.0078,  0.0127,  0.0072,\n",
            "         -0.0095, -0.0036, -0.0017, -0.0106,  0.0084, -0.0136,  0.0003,  0.0037,\n",
            "         -0.0006,  0.0128, -0.0038,  0.0088,  0.0100, -0.0110,  0.0103,  0.0005,\n",
            "         -0.0073, -0.0059,  0.0091,  0.0045, -0.0091,  0.0093, -0.0110,  0.0054,\n",
            "          0.0024,  0.0032,  0.0010,  0.0004, -0.0085,  0.0145,  0.0065, -0.0132,\n",
            "          0.0065, -0.0055,  0.0058,  0.0098,  0.0068,  0.0121,  0.0025, -0.0077,\n",
            "         -0.0085,  0.0083,  0.0050,  0.0016,  0.0092,  0.0097, -0.0014,  0.0034,\n",
            "         -0.0127, -0.0133,  0.0016, -0.0096, -0.0148,  0.0081, -0.0061, -0.0081],\n",
            "        [-0.0005,  0.0126, -0.0137, -0.0104,  0.0092,  0.0104, -0.0078, -0.0115,\n",
            "         -0.0076, -0.0122, -0.0049, -0.0021,  0.0114, -0.0146,  0.0071,  0.0075,\n",
            "         -0.0082,  0.0024,  0.0010,  0.0100,  0.0071, -0.0035,  0.0090, -0.0134,\n",
            "         -0.0108, -0.0005, -0.0009,  0.0004,  0.0122, -0.0023, -0.0136, -0.0040,\n",
            "         -0.0156,  0.0063, -0.0147, -0.0154,  0.0101,  0.0034, -0.0088,  0.0037,\n",
            "          0.0151, -0.0005, -0.0080, -0.0140,  0.0071,  0.0078,  0.0039,  0.0057,\n",
            "          0.0063,  0.0041, -0.0140, -0.0140,  0.0066, -0.0068,  0.0084,  0.0050,\n",
            "          0.0126, -0.0109, -0.0044,  0.0120, -0.0003, -0.0006,  0.0076,  0.0044]],\n",
            "       device='cuda:0', grad_fn=<SplitWithSizesBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhuQyU7AYE6K"
      },
      "source": [
        "## Import The Data\n",
        "The first step before training the model is to import the data. We will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) which is like the Hello World dataset of machine learning. \n",
        "\n",
        "Besides importing the data, we will also do a few more things:\n",
        "- We will tranform the data into tensors using the `transforms` module\n",
        "- We will use `DataLoader` to build convenient data loaders or what are referred to as iterators, which makes it easy to efficiently feed data in batches to deep learning models. \n",
        "- As hinted above, we will also create batches of the data by setting the `batch` parameter inside the data loader. Notice we use batches of `32` in this tutorial but you can change it to `64` if you like. I encourage you to experiment with different batches."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXrEDJOabgMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSjjLXrOVWBy",
        "outputId": "47502e82-f178-452b-995f-8a469670a471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "## transformations\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "## download and load training dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "## download and load testing dataset\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3643813.85it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 57582.77it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 973571.63it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21777.08it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nZwZukWXUDn"
      },
      "source": [
        "## Exploring the Data\n",
        "As a practioner and researcher, I am always spending a bit of time and effort exploring and understanding the dataset. It's fun and this is a good practise to ensure that everything is in order. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW_loWKga7CH"
      },
      "source": [
        "Let's check what the train and test dataset contains. I will use `matplotlib` to print out some of the images from our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWd9Pt1Ca6K9",
        "outputId": "1c02a3b5-f5bb-4c51-a999-52d0472f43af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "## functions to show an image\n",
        "def imshow(img):\n",
        "    #img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "## get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "## show images\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADLCAYAAABgQVj0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFWll5Lseiht"
      },
      "source": [
        "**EXERCISE:** Try to understand what the code above is doing. This will help you to better understand your dataset before moving forward. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9mXAVmRvhrq"
      },
      "source": [
        "Let's check the dimensions of a batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNFKWz1GZ4R5",
        "outputId": "cc1fd627-b8b0-42d4-d1a7-cd1eeaefc7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "for images, labels in trainloader:\n",
        "    print(\"Image batch dimensions:\", images.shape)\n",
        "    print(\"Image label dimensions:\", labels.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image batch dimensions: torch.Size([32, 1, 28, 28])\n",
            "Image label dimensions: torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmaCTw5tXowR"
      },
      "source": [
        "## The Model\n",
        "Now using the classical deep learning framework pipeline, let's build the 1 convolutional layer model. \n",
        "\n",
        "Here are a few notes for those who are beginning with PyTorch:\n",
        "- The model below consists of an `__init__()` portion which is where you include the layers and components of the neural network. In our model, we have a convolutional layer denoted by `nn.Conv2d(...)`. We are dealing with an image dataset that is in a grayscale so we only need one channel going in, hence `in_channels=1`. We hope to get a nice representation of this layer, so we use `out_channels=32`. Kernel size is 3, and for the rest of parameters we use the default values which you can find [here](https://pytorch.org/docs/stable/nn.html?highlight=conv2d#conv2d). \n",
        "- We use 2 back to back dense layers or what we refer to as linear transformations to the incoming data. Notice for `d1` I have a dimension which looks like it came out of nowhere. 128 represents the size we want as output and the (`26*26*32`) represents the dimension of the incoming data. If you would like to find out how to calculate those numbers refer to the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html?highlight=linear#conv2d). In short, the convolutional layer transforms the input data into a specific dimension that has to be considered in the linear layer. The same applies for the second linear transformation (`d2`) where the dimension of the output of the previous linear layer was added as `in_features=128`, and `10` is just the size of the output which also corresponds to the number of classes.\n",
        "- After each one of those layers, we also apply an activation function such as `ReLU`. For prediction purposes, we then apply a `softmax` layer to the last transformation and return the output of that.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IYnV4ZBa3cJ"
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        # 28x28x1 => 26x26x32\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "        self.d1 = nn.Linear(26 * 26 * 32, 128)\n",
        "        self.d2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 32x1x28x28 => 32x32x26x26\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # flatten => 32 x (32*26*26)\n",
        "        x = x.flatten(start_dim = 1)\n",
        "\n",
        "        # 32 x (32*26*26) => 32x128\n",
        "        x = self.d1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # logits => 32x10\n",
        "        logits = self.d2(x)\n",
        "        out = F.softmax(logits, dim=1)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evsFbkq_X6bc"
      },
      "source": [
        "As I have done in my previous tutorials, I always encourage to test the model with 1 batch to ensure that the output dimensions are what we expect. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1poxFYqftKov",
        "outputId": "0a845d9b-54c8-43b9-c3d6-1abc1b7a4f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "## test the model with 1 batch\n",
        "model = MyModel()\n",
        "for images, labels in trainloader:\n",
        "    print(\"batch size:\", images.shape)\n",
        "    out = model(images)\n",
        "    print(out.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch size: torch.Size([32, 1, 28, 28])\n",
            "torch.Size([32, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h_3eZQRHV_P"
      },
      "source": [
        "## Training the Model\n",
        "Now we are ready to train the model but before that we are going to setup a loss function, an optimizer and a function to compute accuracy of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_0Vjq2RHlph"
      },
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MyModel()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44IdrNNeIi_I"
      },
      "source": [
        "## compute accuracy\n",
        "def get_accuracy(logit, target, batch_size):\n",
        "    ''' Obtain accuracy for training round '''\n",
        "    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "    accuracy = 100.0 * corrects/batch_size\n",
        "    return accuracy.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK3EcuIOISSR"
      },
      "source": [
        "Now it's time for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E59hwZlAIVcL",
        "outputId": "ab16b14b-8a6e-4568-8500-2f2f5b447a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    train_running_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "\n",
        "    model = model.train()\n",
        "\n",
        "    ## training step\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        ## forward + backprop + loss\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        ## update model params\n",
        "        optimizer.step()\n",
        "\n",
        "        train_running_loss += loss.detach().item()\n",
        "        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n",
        "    \n",
        "    model.eval()\n",
        "    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n",
        "          %(epoch, train_running_loss / i, train_acc/i))        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 1.4901 | Train Accuracy: 96.97\n",
            "Epoch: 1 | Loss: 1.4808 | Train Accuracy: 97.90\n",
            "Epoch: 2 | Loss: 1.4767 | Train Accuracy: 98.34\n",
            "Epoch: 3 | Loss: 1.4748 | Train Accuracy: 98.55\n",
            "Epoch: 4 | Loss: 1.4725 | Train Accuracy: 98.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuZxfQc1UIU-"
      },
      "source": [
        "We can also compute accuracy on the testing dataset to see how well the model performs on the image classificaiton task. As you can see below, our basic CNN model is performing very well on the MNIST classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU5WR0BTUHv1",
        "outputId": "e0f48883-e06a-4108-a933-0f33b2e56b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc = 0.0\n",
        "for i, (images, labels) in enumerate(testloader, 0):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    test_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n",
        "        \n",
        "print('Test Accuracy: %.2f'%( test_acc/i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 98.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZz7LAewgGAK"
      },
      "source": [
        "**EXERCISE:** As a way to practise, try to include the testing part inside the code where I was outputing the training accuracy, so that you can also keep testing the model on the testing data as you proceed with the training steps. This is useful as sometimes you don't want to wait until your model has completed training to actually test the model with the testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLQlqGPsVjOB"
      },
      "source": [
        "## Final Words\n",
        "That's it for this tutorial! Congratulations! You are now able to implement a basic CNN model in PyTorch for image classification. If you would like, you can further extend the CNN model by adding more convolution layers and max pooling, but as you saw, you don't really need it here as results look good. If you are interested in implementing a similar image classification model using RNNs see the references below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztAiTq9HcS_H"
      },
      "source": [
        "## References\n",
        "- [Building RNNs is Fun with PyTorch and Google Colab](https://colab.research.google.com/drive/1NVuWLZ0cuXPAtwV4Fs2KZ2MNla0dBUas)\n",
        "- [CNN Basics with PyTorch by Sebastian Raschka](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-basic.ipynb)\n",
        "- [Tensorflow 2.0 Quickstart for experts](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/advanced.ipynb#scrollTo=DUNzJc4jTj6G) "
      ]
    }
  ]
}